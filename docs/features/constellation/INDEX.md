# Constellation Architecture â€” Feature Index

## Overview

The Constellation Architecture extends LaminarDB from an embedded single-process streaming engine to a multi-node distributed system without changing operator code. Three deployment tiers share the same SQL, the same operators, the same state traits â€” only the infrastructure configuration changes.

**Design Philosophy:**
- **Ring 0 is sacred.** The hot path (sub-500ns per operation) uses only synchronous, zero-allocation, zero-lock data structures.
- **Location-transparent state.** Operator code calls `state.get(key)` regardless of deployment tier.
- **Object storage is the single source of truth.** Every node is ephemeral. Durable state lives in S3/GCS/local filesystem via the `object_store` crate.

**Deployment Tiers:**
- **Tier 1 â€” Embedded Single-Process:** Library linked into your application
- **Tier 2 â€” Multi-Partition Single Machine:** Thread-per-core, NUMA-aware
- **Tier 3 â€” Constellation:** Multiple nodes, gossip discovery, Raft coordination

**Naming Conventions:**

| Concept | Name | Rationale |
|---------|------|-----------|
| Single node | Star | Self-sufficient unit |
| Multi-node group | Constellation | Peers forming recognizable pattern |
| Node discovery | Starfinding | Discovery of peers |
| Partition map | Star Chart | Map of partition â†’ node assignments |
| Node joining | Rising | Astronomical term for appearance |
| Node leaving | Setting | Astronomical term for departure |
| Multi-region (future) | Galaxy | Contains multiple constellations |

## Feature Summary

| Phase | Total | Draft | Superseded | In Progress | Done |
|-------|-------|-------|------------|-------------|------|
| Phase 6a â€” Partition-Parallel Embedded | 27 | 3 | 2 | 0 | 20 |
| Phase 6b â€” Constellation Foundation | 14 | 14 | 0 | 0 | 0 |
| Phase 6c â€” Production Hardening | 10 | 9 | 1 | 0 | 0 |
| **Total** | **51** | **26** | **3** | **0** | **20** |

## Key Dependencies

See [DEPENDENCIES.md](DEPENDENCIES.md) for the full project dependency graph.

**Constellation prerequisites (from earlier phases):**
- F-STREAM-002 (SPSC Channel) â€” barrier protocol uses same channels
- F013 (Thread-Per-Core) â€” cross-partition aggregation
- F006B (Production SQL Parser) â€” CREATE LOOKUP TABLE DDL
- F005B (Advanced DataFusion) â€” lookup join plan nodes
- F025/F026 (Kafka Source/Sink) â€” Kafka discovery, transactional sinks
- F027 (PostgreSQL CDC) â€” CDC-to-cache adapter
- F-CKP-001â€“009 (Unified Checkpoint) â€” foundation for distributed checkpointing

---

## Phase 6a: Partition-Parallel Embedded

> **Goal**: State store infrastructure, checkpoint barriers, lookup tables with foyer caching, source pushdown, SQL integration, and baseline benchmarks. Single-process multi-partition with object store checkpoints.

### State Store Architecture

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-STATE-001 | Revised StateStore Trait | âœ… | P0 | M | [Link](state/F-STATE-001-state-store-trait.md) |
| F-STATE-002 | InMemoryStateStore (AHashMap/BTreeSet) | âœ… | P0 | S | [Link](state/F-STATE-002-inmemory-state-store.md) |
| F-STATE-003 | ~~MmapStateStore~~ | âŒ | â€” | â€” | [Link](state/F-STATE-003-mmap-state-store.md) |
| F-STATE-004 | ~~Pluggable Snapshot Strategies~~ | âŒ | â€” | â€” | [Link](state/F-STATE-004-pluggable-snapshots.md) |

### Distributed Checkpoint (Embedded)

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-DCKP-001 | Checkpoint Barrier Protocol | âœ… | P0 | M | [Link](checkpoint/F-DCKP-001-barrier-protocol.md) |
| F-DCKP-002 | Barrier Alignment | âœ… | P0 | M | [Link](checkpoint/F-DCKP-002-barrier-alignment.md) |
| F-DCKP-003 | Object Store Checkpoint Layout | âœ… | P0 | S | [Link](checkpoint/F-DCKP-003-object-store-layout.md) |
| F-DCKP-004 | ObjectStoreCheckpointer | âœ… | P0 | M | [Link](checkpoint/F-DCKP-004-object-store-checkpointer.md) |
| F-DCKP-005 | Recovery Manager | âœ… | P0 | L | [Link](checkpoint/F-DCKP-005-recovery-manager.md) |

### Lookup Tables

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-LOOKUP-001 | LookupTable Trait & Strategy | âœ… | P0 | M | [Link](lookup/F-LOOKUP-001-lookup-table-trait.md) |
| F-LOOKUP-002 | LookupSource Trait | âœ… | P0 | M | [Link](lookup/F-LOOKUP-002-lookup-source-trait.md) |
| F-LOOKUP-003 | Predicate Types | âœ… | P0 | S | [Link](lookup/F-LOOKUP-003-predicate-types.md) |
| F-LOOKUP-004 | foyer In-Memory Cache (Ring 0) | âœ… | P0 | M | [Link](lookup/F-LOOKUP-004-foyer-memory-cache.md) |
| F-LOOKUP-005 | foyer Hybrid Cache (Ring 1) | âœ… | P1 | M | [Link](lookup/F-LOOKUP-005-foyer-hybrid-cache.md) |
| F-LOOKUP-006 | CDC-to-Cache Adapter | âœ… | P0 | S | [Link](lookup/F-LOOKUP-006-cdc-cache-adapter.md) |
| F-LOOKUP-007 | PostgresLookupSource | âœ… | P1 | M | [Link](lookup/F-LOOKUP-007-postgres-lookup-source.md) |
| F-LOOKUP-008 | ParquetLookupSource | ğŸ“ | P2 | M | [Link](lookup/F-LOOKUP-008-parquet-lookup-source.md) |
| F-LOOKUP-010 | Remove RocksDB Dependency | ğŸ“ | P0 | S | [Link](lookup/F-LOOKUP-010-rocksdb-removal.md) |

### Secondary Indexes

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-IDX-001 | redb Secondary Indexes | ğŸ“ | P1 | M | [Link](indexes/F-IDX-001-redb-secondary-indexes.md) |

### Deployment Profiles

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-PROFILE-001 | Deployment Profiles | âœ… | P0 | M | [Link](profiles/F-PROFILE-001-deployment-profiles.md) |

### Cross-Partition Aggregation

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-XAGG-001 | Cross-Partition Lock-Free HashMap | âœ… | P1 | M | [Link](aggregation/F-XAGG-001-cross-partition-hashmap.md) |

### Exactly-Once (Source Layer)

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-E2E-001 | Source Offset Checkpoint | ğŸ“ | P0 | M | [Link](exactly-once/F-E2E-001-source-offset-checkpoint.md) |

### SQL Integration

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-LSQL-001 | CREATE LOOKUP TABLE DDL | âœ… | P0 | M | [Link](sql/F-LSQL-001-create-lookup-table.md) |
| F-LSQL-002 | Lookup Join Plan Node | âœ… | P0 | L | [Link](sql/F-LSQL-002-lookup-join-plan-node.md) |
| F-LSQL-003 | Predicate Splitting & Pushdown | âœ… | P1 | M | [Link](sql/F-LSQL-003-predicate-splitting.md) |

### Performance Benchmarks (Baseline)

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-PERF-001 | StateStore Microbenchmarks | âœ… | P0 | S | [Link](benchmarks/F-PERF-001-state-store-benchmarks.md) |
| F-PERF-002 | Cache Hit/Miss Ratio Benchmarks | ğŸ“ | P1 | S | [Link](benchmarks/F-PERF-002-cache-benchmarks.md) |
| F-PERF-003 | Checkpoint Cycle Benchmark | ğŸ“ | P1 | S | [Link](benchmarks/F-PERF-003-checkpoint-cycle-benchmark.md) |
| F-PERF-005 | Lookup Join Throughput | ğŸ“ | P1 | S | [Link](benchmarks/F-PERF-005-lookup-join-throughput.md) |

---

## Phase 6b: Constellation Foundation

> **Goal**: Multi-node discovery (gossip), Raft-based metadata consensus, partition ownership with epoch fencing, distributed checkpointing, partitioned lookups, cross-node aggregation, inter-node RPC.

### Discovery

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-DISC-001 | Discovery Trait & Static Discovery | ğŸ“ | P0 | M | [Link](discovery/F-DISC-001-static-discovery.md) |
| F-DISC-002 | Gossip Discovery (chitchat) | ğŸ“ | P0 | L | [Link](discovery/F-DISC-002-gossip-discovery.md) |
| F-DISC-003 | Kafka Group Discovery | ğŸ“ | P2 | M | [Link](discovery/F-DISC-003-kafka-discovery.md) |

### Coordination

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-COORD-001 | Raft Metadata Consensus | ğŸ“ | P0 | XL | [Link](coordination/F-COORD-001-raft-metadata.md) |
| F-COORD-002 | Constellation Orchestration | ğŸ“ | P0 | L | [Link](coordination/F-COORD-002-constellation-orchestration.md) |

### Partition Ownership

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-EPOCH-001 | PartitionGuard & Epoch Fencing | ğŸ“ | P0 | M | [Link](partition/F-EPOCH-001-partition-guard.md) |
| F-EPOCH-002 | Partition Assignment Algorithm | ğŸ“ | P1 | M | [Link](partition/F-EPOCH-002-assignment-algorithm.md) |
| F-EPOCH-003 | Partition Reassignment Protocol | ğŸ“ | P0 | L | [Link](partition/F-EPOCH-003-reassignment-protocol.md) |

### Distributed Checkpoint (Constellation)

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-DCKP-008 | Distributed Checkpoint Coordination | ğŸ“ | P0 | XL | [Link](checkpoint/F-DCKP-008-distributed-coordination.md) |

### Constellation Lookup

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-LOOKUP-009 | Partitioned Lookup Strategy | ğŸ“ | P1 | L | [Link](lookup/F-LOOKUP-009-partitioned-lookup.md) |

### Cross-Node Aggregation

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-XAGG-002 | Gossip Partial Aggregates | ğŸ“ | P1 | M | [Link](aggregation/F-XAGG-002-gossip-aggregates.md) |
| F-XAGG-003 | gRPC Aggregate Fan-Out | ğŸ“ | P2 | M | [Link](aggregation/F-XAGG-003-grpc-aggregate-fanout.md) |

### Inter-Node RPC

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-RPC-001 | gRPC Service Definitions | ğŸ“ | P0 | M | [Link](rpc/F-RPC-001-grpc-service-definitions.md) |
| F-RPC-002 | Remote Lookup Service | ğŸ“ | P1 | M | [Link](rpc/F-RPC-002-remote-lookup-service.md) |
| F-RPC-003 | Barrier Forwarding Service | ğŸ“ | P0 | M | [Link](rpc/F-RPC-003-barrier-forwarding.md) |

### Performance Benchmarks (Constellation)

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-PERF-004 | Constellation Checkpoint Benchmark | ğŸ“ | P1 | S | [Link](benchmarks/F-PERF-004-constellation-checkpoint-benchmark.md) |

---

## Phase 6c: Production Hardening

> **Goal**: Unaligned checkpoints, exactly-once sinks, laminardb-server binary with TOML config and HTTP API, hot reload, rolling restarts.

### Advanced Checkpointing

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-DCKP-006 | Unaligned Checkpoints | ğŸ“ | P1 | L | [Link](checkpoint/F-DCKP-006-unaligned-checkpoints.md) |
| F-DCKP-007 | ~~Incremental Mmap Checkpoints~~ | âŒ | â€” | â€” | [Link](checkpoint/F-DCKP-007-incremental-mmap-checkpoints.md) |

### Exactly-Once Sinks

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-E2E-002 | Transactional Sink (2PC) | ğŸ“ | P0 | L | [Link](exactly-once/F-E2E-002-transactional-sink.md) |
| F-E2E-003 | Idempotent Sink | ğŸ“ | P1 | M | [Link](exactly-once/F-E2E-003-idempotent-sink.md) |

### laminardb-server

| ID | Feature | Status | Priority | Effort | Spec |
|----|---------|--------|----------|--------|------|
| F-SERVER-001 | TOML Configuration | ğŸ“ | P0 | M | [Link](server/F-SERVER-001-toml-config.md) |
| F-SERVER-002 | Engine Construction | ğŸ“ | P0 | M | [Link](server/F-SERVER-002-engine-construction.md) |
| F-SERVER-003 | HTTP API | ğŸ“ | P0 | M | [Link](server/F-SERVER-003-http-api.md) |
| F-SERVER-004 | Hot Reload | ğŸ“ | P1 | M | [Link](server/F-SERVER-004-hot-reload.md) |
| F-SERVER-005 | Constellation Server Mode | ğŸ“ | P1 | M | [Link](server/F-SERVER-005-constellation-mode.md) |
| F-SERVER-006 | Graceful Rolling Restart | ğŸ“ | P2 | L | [Link](server/F-SERVER-006-rolling-restart.md) |

---

## Performance Budget

### Ring 0 (Hot Path) â€” < 500ns per event

| Operation | Target | Backend |
|-----------|--------|---------|
| StateStore::get | < 40ns | FxHashMap (in-memory) |
| StateStore::put | < 60ns | FxHashMap (in-memory) |
| StateStore::range | < 200ns | BTreeMap (in-memory) |
| Lookup cache hit (memory-only foyer) | < 500ns | foyer::Cache (sync) |
| Barrier processing | < 50ns | Enum match, no allocation |
| Watermark advance | < 100ns | Comparison + state update |

### Ring 1 (Background) â€” < 100us typical

| Operation | Target | Notes |
|-----------|--------|-------|
| Checkpoint snapshot (rkyv serialize) | < 10ms | Per-partition rkyv zero-copy serialization |
| Lookup cache miss â†’ foyer disk | 10-100us | NVMe read latency |
| redb index point lookup | < 10us | B-tree key lookup |
| Cross-partition aggregate read | 1-10us | papaya lock-free HashMap |

### Ring 2 (Control Plane) â€” milliseconds acceptable

| Operation | Target | Notes |
|-----------|--------|-------|
| Checkpoint upload to S3 | 100ms-10s | Depends on state size |
| Lookup cache miss â†’ Postgres | 1-10ms | Network + query |
| Partition reassignment | 1-30s | Download + restore |
| Gossip convergence | < 2s | chitchat Scuttlebutt |
| Raft consensus round | < 10ms | openraft (3-node) |

---

## Crate Dependencies

All pure Rust. No C++ FFI.

| Crate | Version | Purpose | Ring |
|-------|---------|---------|------|
| `object_store` | 0.13+ | S3/GCS/local checkpoint storage | 2 |
| `foyer` | latest | Hybrid cache for lookup tables | 0/1 |
| `chitchat` | latest | Gossip discovery + failure detection | 2 |
| `openraft` | 0.10+ | Metadata consensus | 2 |
| `papaya` | latest | Lock-free concurrent HashMap | 1 |
| `redb` | latest | Secondary indexes (B-tree, pure Rust) | 1/2 |
| `rkyv` | 0.8+ | Zero-copy state serialization | 1 |
| `tonic` | latest | gRPC inter-node communication | 2 |
| `axum` | latest | HTTP API for laminardb-server | 2 |
| `toml` | latest | Config file parsing | 2 |

**Feature-gated:** `constellation` (chitchat, openraft, tonic), `server` (axum, toml).

**Deferred to Phase 7:** `slatedb` 0.10+ (AsyncStateStore for state exceeding RAM).

---

## Architecture Audit & Fix Plan

An architecture audit was conducted on 2026-02-17 across all 55 constellation feature specs. The audit identified **9 CRITICAL**, **27 HIGH**, **24 MEDIUM**, and **12 LOW** issues.

The prioritized fix plan is in **[ARCHITECTURE-FIX-PLAN.md](ARCHITECTURE-FIX-PLAN.md)**.

**Top 5 systemic issues:**
1. **No snapshot consistency mechanism** â€” O(n) clone at barrier time after mmap supersession (C1)
2. **Incomplete epoch fencing** â€” validates epoch but not owner node_id (C5, C6)
3. **rkyv key encoding breaks index ordering** â€” little-endian doesn't sort correctly (C2)
4. **`unreachable!()` on Ring 0** â€” watermarks hitting barrier aligner crash the partition (C4)
5. **LookupTable lifetime unsoundness** â€” foyer CacheEntry RAII vs borrowed slice (C3)

---

## Decision Log

| Decision | Chosen | Rejected | Rationale |
|----------|--------|----------|-----------|
| State backend default | FxHashMap/BTreeMap (in-memory) | MmapStateStore, SlateDB, RocksDB | FxHashMap ~40ns get vs. mmap ~200ns; rkyv + object_store provides durability without mmap complexity |
| Serialization | rkyv 0.8 | bincode, protobuf | Zero-copy deserialization |
| Discovery | chitchat | Custom gossip | Battle-tested at Quickwit |
| Consensus | openraft | raft-rs, etcd | Pure Rust; async event-driven |
| Lookup cache | foyer | CacheLib (C++) | Production-proven at RisingWave |
| Cross-partition aggregation | papaya | dashmap | Lock-free; higher throughput |
| Checkpoint protocol | Chandy-Lamport | 2PC, Saga | Standard for streaming |
| Cluster model | Node-ownership | Capacity-based | Deterministic latency |
| Server config | TOML | YAML, JSON | Rust ecosystem standard |
| Secondary indexes | redb | RocksDB, sled | Pure Rust, ACID, single-writer/multi-reader fits thread-per-core |
| Deployment model | Named profiles (4 tiers) | Feature flag combos | Validated configs, clear capability matrix |
| Predicate pushdown | Flat enum | Expression tree | Scope containment |
| StateStore get_async | Removed | Keep in base trait | Clean Ring 0 interface |
| Mmap state backend | Removed | MmapStateStore | FxHashMap faster; rkyv + object_store simpler durability |

---

## References

- [Constellation Architecture Research](../../research/constellation-architecture-v3.md)
- [ARCHITECTURE.md](../../ARCHITECTURE.md) â€” Ring model and system design
- [ROADMAP.md](../../ROADMAP.md) â€” Phase timeline
- [STEERING.md](../../STEERING.md) â€” Current priorities
- Apache Flink â€” Chandy-Lamport checkpoint barriers
- RisingWave / Hummock â€” foyer hybrid cache validation
- Feldera / DBSP â€” Simplified fault tolerance
- WarpStream â€” Stateless compute + object storage pattern
